{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import atmos_physics as atmos_physics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(training_split, longitudes, times):\n",
    "    pure_split = int(longitudes*times*training_split)\n",
    "    return math.floor(float(pure_split) / longitudes) * longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = \"/ocean/projects/ees220005p/gmooers/GM_Data/**00000[1]**.nc4\"\n",
    "filepath = \"/ocean/projects/ees220005p/gmooers/GM_Data/**0000012[26]**.nc4\"\n",
    "savepath = \"/ocean/projects/ees220005p/gmooers/GM_Data/training_data/\"\n",
    "n_z_input = 49\n",
    "train_size=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = xr.open_mfdataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = variables.lon  # m\n",
    "y = variables.lat  # m\n",
    "z = variables.z  # m\n",
    "p = variables.p  # hPa\n",
    "rho = variables.rho  # kg/m^3\n",
    "terra = variables.TERRA[:,:n_z_input]\n",
    "SFC_PRES = variables.SFC_REFERENCE_P\n",
    "SKT = variables.SKT\n",
    "n_x = x.size\n",
    "n_y = y.size\n",
    "n_z = z.size\n",
    "n_files = terra.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_lat = np.zeros((n_files, n_y, n_x))\n",
    "sin_lon = np.zeros((n_files, n_y, n_x))\n",
    "cos_lat[:, :, :] = xr.ufuncs.cos(xr.ufuncs.radians(y.values[None, :, None]))\n",
    "sin_lon[:, :, :] = xr.ufuncs.sin(xr.ufuncs.radians(x.values[None, None, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adz = xr.zeros_like(z[:n_z_input]) \n",
    "dz = 0.5*(z[0]+z[1]) \n",
    "adz[0] = 1.\n",
    "\n",
    "for k in range(1,n_z_input-1): # range doesn't include stopping number\n",
    "    adz[k] = 0.5*(z[k+1]-z[k-1])/dz\n",
    "\n",
    "adz[n_z_input-1] = (z[n_z_input-1]-z[n_z_input-2])/dz\n",
    "rho_dz = adz*dz*rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin = variables.TABS_SIGMA[:,:n_z_input] #originally just called tabs\n",
    "Qrad = variables.QRAD_SIGMA[:,:n_z_input] / 86400\n",
    "qt = (variables.QV_SIGMA[:,:n_z_input] + variables.QC_SIGMA[:,:n_z_input] + variables.QI_SIGMA[:,:n_z_input]) / 1000.0 # originally called qt\n",
    "qp = variables.QP_SIGMA[:,:n_z_input] / 1000.0\n",
    "q_auto_out = -1.0*variables.QP_MICRO_SIGMA[:,:n_z_input] / 1000.0\n",
    "qpflux_z_coarse = variables.RHOQPW_SIGMA[:,:n_z_input] / 1000.0\n",
    "T_adv_out = variables.T_FLUX_Z_OUT_SUBGRID_SIGMA[:,:n_z_input]     #originally tflux_z\n",
    "q_adv_out = variables.Q_FLUX_Z_OUT_SUBGRID_SIGMA[:,:n_z_input] / 1000.0 #originally qtflux_z\n",
    "qpflux_z = variables.QP_FLUX_Z_OUT_SUBGRID_SIGMA[:,:n_z_input] / 1000.0 \n",
    "w = variables.W[:,:n_z_input]  # m/s\n",
    "precip = variables.PREC_SIGMA[:,:n_z_input]  # precipitation flux kg/m^2/s\n",
    "cloud_qt_flux = variables.SED_SIGMA[:,:n_z_input] / 1000.0\n",
    "cloud_lat_heat_flux = variables.LSED_SIGMA[:,:n_z_input] \n",
    "qpflux_diff_coarse_z = variables.RHOQPS_SIGMA[:,:n_z_input] / 1000.0  # SGS qp flux kg/m^2/s Note that I need this variable\n",
    "#q_auto_out = - dqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pr = 1.0 / (atmos_physics.tprmax - atmos_physics.tprmin)\n",
    "omp = np.maximum(0.0, np.minimum(1.0, (Tin - atmos_physics.tprmin) * a_pr))\n",
    "fac = (atmos_physics.L + atmos_physics.Lf * (1.0 - omp)) / atmos_physics.cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sed_fluxc_out = ((atmos_physics.L + atmos_physics.Lf) * cloud_qt_flux + cloud_lat_heat_flux) / atmos_physics.Lf\n",
    "q_sed_fluxi_out = - (atmos_physics.L * cloud_qt_flux + cloud_lat_heat_flux) / atmos_physics.Lf\n",
    "q_sed_flux_tot  = cloud_qt_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfac_dz = np.zeros((n_files, n_z_input, n_y, n_x))\n",
    "for k in range(n_z_input - 1):\n",
    "    kb = max(0, k - 1)\n",
    "    dfac_dz[:, k, :, :] = (fac[:, k + 1, :, :] - fac[:, k, :, :]) / rho_dz[k, :] * rho[:, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tout = dfac_dz * (qpflux_z_coarse + qpflux_diff_coarse_z - precip) / rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = get_train_test_split(train_size, n_x*n_y, n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict_train = {}\n",
    "my_dict_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 49, 426, 768)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin_new = Tin.transpose(\"z\",\"time\",\"lat\",\"lon\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 2, 426, 768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tin_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(Tin[0,:,:,:], Tin_new[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin_new_something = np.reshape(Tin_new, (n_z_input, n_files*n_y*n_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 654336)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tin_new_something.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin_new_something_new = np.reshape(Tin_new_something, (n_z_input, n_files, n_y, n_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(Tin_new_something_new[:,0,:,:], Tin_new[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "654336/ (n_y*n_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin_new = Tin.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "Tin_new_something = np.reshape(Tin_new, (n_z_input, n_files*n_y*n_x))\n",
    "qin = qt.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "qin = np.reshape(qin, (n_z_input, n_files*n_y*n_x))\n",
    "Tout = Tout.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "Tout = np.reshape(Tout, (n_z_input, n_files*n_y*n_x))\n",
    "T_adv_out = T_adv_out.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "T_adv_out = np.reshape(T_adv_out, (n_z_input, n_files*n_y*n_x))\n",
    "q_adv_out = q_adv_out.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "q_adv_out = np.reshape(q_adv_out, (n_z_input, n_files*n_y*n_x))\n",
    "q_auto_out = q_auto_out.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "q_auto_out = np.reshape(q_auto_out, (n_z_input, n_files*n_y*n_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin_new = Tin.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "Tin_new_something = np.reshape(Tin_new, (n_z_input, n_files*n_y*n_x))\n",
    "my_dict_train[\"Tin\"] = ((\"z\",\"sample\"), Tin_new_something[...,:split_index])\n",
    "my_dict_test[\"Tin\"] = ((\"z\",\"sample\"), Tin_new_something[...,split_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 654336)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tin_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 49, 426, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(Tin_new[:,0,:,:], Tin_new_something[:,:split_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin = Tin.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "Tin = np.reshape(Tin, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"Tin\"] = ((\"z\",\"lat\",\"sample\"), Tin[...,:split_index])\n",
    "my_dict_test[\"Tin\"] = ((\"z\",\"lat\",\"sample\"), Tin[...,split_index:])\n",
    "\n",
    "qin = qt.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "qin = np.reshape(qin, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"qin\"] = ((\"z\",\"lat\",\"sample\"), qin[...,:split_index])\n",
    "my_dict_test[\"qin\"] = ((\"z\",\"lat\",\"sample\"), qin[...,split_index:])\n",
    "\n",
    "Tout = Tout.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "Tout = np.reshape(Tout, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"Tout\"] = ((\"z\",\"lat\",\"sample\"), Tout[...,:split_index])\n",
    "my_dict_test[\"Tout\"] = ((\"z\",\"lat\",\"sample\"), Tout[...,split_index:])\n",
    "\n",
    "T_adv_out = T_adv_out.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "T_adv_out = np.reshape(T_adv_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"T_adv_out\"] = ((\"z\",\"lat\",\"sample\"), T_adv_out[...,:split_index])\n",
    "my_dict_test[\"T_adv_out\"] = ((\"z\",\"lat\",\"sample\"), T_adv_out[...,split_index:])\n",
    "\n",
    "q_adv_out = q_adv_out.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "q_adv_out = np.reshape(q_adv_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_adv_out\"] = ((\"z\",\"lat\",\"sample\"), q_adv_out[...,:split_index])\n",
    "my_dict_test[\"q_adv_out\"] = ((\"z\",\"lat\",\"sample\"), q_adv_out[...,split_index:])\n",
    "\n",
    "q_auto_out = q_auto_out.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "q_auto_out = np.reshape(q_auto_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_adv_out\"] = ((\"z\",\"lat\",\"sample\"), q_adv_out[...,:split_index])\n",
    "my_dict_test[\"q_adv_out\"] = ((\"z\",\"lat\",\"sample\"), q_adv_out[...,split_index:])\n",
    "\n",
    "q_sed_flux_tot = q_sed_flux_tot.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "q_sed_flux_tot = np.reshape(q_sed_flux_tot, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_sed_flux_tot\"] = ((\"z\",\"lat\",\"sample\"), q_sed_flux_tot[...,:split_index])\n",
    "my_dict_test[\"q_sed_flux_tot\"] = ((\"z\",\"lat\",\"sample\"), q_sed_flux_tot[...,split_index:])\n",
    "\n",
    "q_sed_fluxi_out = q_sed_fluxi_out.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "q_sed_fluxi_out = np.reshape(q_sed_fluxi_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_sed_fluxi_out\"] = ((\"z\",\"lat\",\"sample\"), q_sed_fluxi_out[...,:split_index])\n",
    "my_dict_test[\"q_sed_fluxi_out\"] = ((\"z\",\"lat\",\"sample\"), q_sed_fluxi_out[...,split_index:])\n",
    "\n",
    "q_sed_fluxc_out = q_sed_fluxc_out.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "q_sed_fluxc_out = np.reshape(q_sed_fluxc_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_sed_fluxc_out\"] = ((\"z\",\"lat\",\"sample\"), q_sed_fluxc_out[...,:split_index])\n",
    "my_dict_test[\"q_sed_fluxc_out\"] = ((\"z\",\"lat\",\"sample\"), q_sed_fluxc_out[...,split_index:])\n",
    "\n",
    "terra = terra.transpose(\"z\",\"time\",\"lat\",\"lon\").values\n",
    "terra = np.reshape(terra, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"terra\"] = ((\"z\",\"lat\",\"sample\"), terra[...,:split_index])\n",
    "my_dict_test[\"terra\"] = ((\"z\",\"lat\",\"sample\"), terra[...,split_index:])\n",
    "\n",
    "sfc_pres = SFC_PRES.transpose(\"time\",\"lat\",\"lon\").values\n",
    "sfc_pres = np.reshape(sfc_pres, (n_y, n_files*n_x))\n",
    "my_dict_train[\"sfc_pres\"] = ((\"lat\",\"sample\"), sfc_pres[...,:split_index])\n",
    "my_dict_test[\"sfc_pres\"] = ((\"lat\",\"sample\"), sfc_pres[...,split_index:])\n",
    "\n",
    "skt = SKT.transpose(\"time\",\"lat\",\"lon\").values\n",
    "skt = np.reshape(skt, (n_y, n_files*n_x))\n",
    "my_dict_train[\"skt\"] = ((\"lat\",\"sample\"), skt[...,:split_index])\n",
    "my_dict_test[\"skt\"] = ((\"lat\",\"sample\"), skt[...,split_index:])\n",
    "\n",
    "cos_lat = np.expand_dims(cos_lat, axis=0)\n",
    "cos_lat = np.moveaxis(cos_lat, 2, 3)\n",
    "cos_lat = np.reshape(cos_lat, (1, n_y, -1)).squeeze()\n",
    "my_dict_train[\"cos_lat\"] = ((\"lat\",\"sample\"), cos_lat[...,:split_index])\n",
    "my_dict_test[\"cos_lat\"] = ((\"lat\",\"sample\"), cos_lat[...,split_index:])\n",
    "\n",
    "sin_lon = np.expand_dims(sin_lon, axis=0)\n",
    "sin_lon = np.moveaxis(sin_lon, 2, 3)\n",
    "sin_lon = np.reshape(sin_lon, (1, n_y, -1)).squeeze()\n",
    "my_dict_train[\"sin_lon\"] = ((\"lat\",\"sample\"), sin_lon[...,:split_index])\n",
    "my_dict_test[\"sin_lon\"] = ((\"lat\",\"sample\"), sin_lon[...,split_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = xr.Dataset(\n",
    "    my_dict_train,\n",
    "    coords={\n",
    "        \"z\": z[:n_z_input].values,\n",
    "        \"lat\": y.values,\n",
    "        \"sample\": np.arange(0,n_files*len(x.values), 1)[:split_index],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = xr.Dataset(\n",
    "    my_dict_test,\n",
    "    coords={\n",
    "        \"z\": z[:n_z_input].values,\n",
    "        \"lat\": y.values,\n",
    "        \"sample\": np.arange(0,n_files*len(x.values), 1)[split_index:],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.to_netcdf(savepath + \"_train.nc\")\n",
    "ds_test.to_netcdf(savepath + \"_test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
