{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import atmos_physics as atmos_physics\n",
    "import math\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(training_split, longitudes, times):\n",
    "    pure_split = int(longitudes*times*training_split)\n",
    "    return math.floor(float(pure_split) / longitudes) * longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = \"/ocean/projects/ees220005p/gmooers/GM_Data/**00000[1]**.nc4\"\n",
    "filepath = \"/ocean/projects/ees220005p/gmooers/GM_Data/**0000012[26]**.nc4\"\n",
    "savepath = \"/ocean/projects/ees220005p/gmooers/GM_Data/training_data/\"\n",
    "n_z_input = 49\n",
    "train_size=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = xr.open_mfdataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = variables.lon  # m\n",
    "y = variables.lat  # m\n",
    "z = variables.z  # m\n",
    "p = variables.p  # hPa\n",
    "rho = variables.rho  # kg/m^3\n",
    "terra = variables.TERRA[:,:n_z_input]\n",
    "SFC_PRES = variables.SFC_REFERENCE_P\n",
    "SKT = variables.SKT\n",
    "n_x = x.size\n",
    "n_y = y.size\n",
    "n_z = z.size\n",
    "n_files = terra.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_lat = np.zeros((n_files, n_y, n_x))\n",
    "sin_lon = np.zeros((n_files, n_y, n_x))\n",
    "cos_lat[:, :, :] = xr.ufuncs.cos(xr.ufuncs.radians(y.values[None, :, None]))\n",
    "sin_lon[:, :, :] = xr.ufuncs.sin(xr.ufuncs.radians(x.values[None, None, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adz = xr.zeros_like(z[:n_z_input]) \n",
    "dz = 0.5*(z[0]+z[1]) \n",
    "adz[0] = 1.\n",
    "\n",
    "for k in range(1,n_z_input-1): # range doesn't include stopping number\n",
    "    adz[k] = 0.5*(z[k+1]-z[k-1])/dz\n",
    "\n",
    "adz[n_z_input-1] = (z[n_z_input-1]-z[n_z_input-2])/dz\n",
    "rho_dz = adz*dz*rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_dz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin = variables.TABS_SIGMA[:,:n_z_input] #originally just called tabs\n",
    "Qrad = variables.QRAD_SIGMA[:,:n_z_input] / 86400\n",
    "qt = (variables.QV_SIGMA[:,:n_z_input] + variables.QC_SIGMA[:,:n_z_input] + variables.QI_SIGMA[:,:n_z_input]) / 1000.0 # originally called qt\n",
    "qp = variables.QP_SIGMA[:,:n_z_input] / 1000.0\n",
    "q_auto_out = -1.0*variables.QP_MICRO_SIGMA[:,:n_z_input] / 1000.0\n",
    "qpflux_z_coarse = variables.RHOQPW_SIGMA[:,:n_z_input] / 1000.0\n",
    "T_adv_out = variables.T_FLUX_Z_OUT_SUBGRID_SIGMA[:,:n_z_input]     #originally tflux_z\n",
    "q_adv_out = variables.Q_FLUX_Z_OUT_SUBGRID_SIGMA[:,:n_z_input] / 1000.0 #originally qtflux_z\n",
    "qpflux_z = variables.QP_FLUX_Z_OUT_SUBGRID_SIGMA[:,:n_z_input] / 1000.0 \n",
    "w = variables.W[:,:n_z_input]  # m/s\n",
    "precip = variables.PREC_SIGMA[:,:n_z_input]  # precipitation flux kg/m^2/s\n",
    "cloud_qt_flux = variables.SED_SIGMA[:,:n_z_input] / 1000.0\n",
    "cloud_lat_heat_flux = variables.LSED_SIGMA[:,:n_z_input] \n",
    "qpflux_diff_coarse_z = variables.RHOQPS_SIGMA[:,:n_z_input] / 1000.0  # SGS qp flux kg/m^2/s Note that I need this variable\n",
    "#q_auto_out = - dqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pr = 1.0 / (atmos_physics.tprmax - atmos_physics.tprmin)\n",
    "omp = np.maximum(0.0, np.minimum(1.0, (Tin - atmos_physics.tprmin) * a_pr))\n",
    "fac = (atmos_physics.L + atmos_physics.Lf * (1.0 - omp)) / atmos_physics.cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sed_fluxc_out = ((atmos_physics.L + atmos_physics.Lf) * cloud_qt_flux + cloud_lat_heat_flux) / atmos_physics.Lf\n",
    "q_sed_fluxi_out = - (atmos_physics.L * cloud_qt_flux + cloud_lat_heat_flux) / atmos_physics.Lf\n",
    "q_sed_flux_tot  = cloud_qt_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfac_dz = np.zeros((n_files, n_z_input, n_y, n_x))\n",
    "for k in range(n_z_input - 1):\n",
    "    kb = max(0, k - 1)\n",
    "    dfac_dz[:, k, :, :] = (fac[:, k + 1, :, :] - fac[:, k, :, :]) / rho_dz[k, :] * rho[:, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tout = dfac_dz * (qpflux_z_coarse + qpflux_diff_coarse_z - precip) / rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = get_train_test_split(train_size, n_x, n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict_train = {}\n",
    "my_dict_test = {}\n",
    "my_weight_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tin = Tin.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "Tin = np.reshape(Tin, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"Tin\"] = ((\"z\",\"lat\",\"sample\"), Tin[...,:split_index])\n",
    "my_dict_test[\"Tin\"] = ((\"z\",\"lat\",\"sample\"), Tin[...,split_index:])\n",
    "\n",
    "qin = qt.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "qin = np.reshape(qin, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"qin\"] = ((\"z\",\"lat\",\"sample\"), qin[...,:split_index])\n",
    "my_dict_test[\"qin\"] = ((\"z\",\"lat\",\"sample\"), qin[...,split_index:])\n",
    "\n",
    "Tout = Tout.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "Tout = np.reshape(Tout, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"Tout\"] = ((\"z\",\"lat\",\"sample\"), Tout[...,:split_index])\n",
    "my_dict_test[\"Tout\"] = ((\"z\",\"lat\",\"sample\"), Tout[...,split_index:])\n",
    "\n",
    "T_adv_out = T_adv_out.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "T_adv_out = np.reshape(T_adv_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"T_adv_out\"] = ((\"z\",\"lat\",\"sample\"), T_adv_out[...,:split_index])\n",
    "my_dict_test[\"T_adv_out\"] = ((\"z\",\"lat\",\"sample\"), T_adv_out[...,split_index:])\n",
    "\n",
    "q_adv_out = q_adv_out.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "q_adv_out = np.reshape(q_adv_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_adv_out\"] = ((\"z\",\"lat\",\"sample\"), q_adv_out[...,:split_index])\n",
    "my_dict_test[\"q_adv_out\"] = ((\"z\",\"lat\",\"sample\"), q_adv_out[...,split_index:])\n",
    "\n",
    "q_auto_out = q_auto_out.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "q_auto_out = np.reshape(q_auto_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_adv_out\"] = ((\"z\",\"lat\",\"sample\"), q_adv_out[...,:split_index])\n",
    "my_dict_test[\"q_adv_out\"] = ((\"z\",\"lat\",\"sample\"), q_adv_out[...,split_index:])\n",
    "\n",
    "q_sed_flux_tot = q_sed_flux_tot.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "q_sed_flux_tot = np.reshape(q_sed_flux_tot, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_sed_flux_tot\"] = ((\"z\",\"lat\",\"sample\"), q_sed_flux_tot[...,:split_index])\n",
    "my_dict_test[\"q_sed_flux_tot\"] = ((\"z\",\"lat\",\"sample\"), q_sed_flux_tot[...,split_index:])\n",
    "\n",
    "q_sed_fluxi_out = q_sed_fluxi_out.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "q_sed_fluxi_out = np.reshape(q_sed_fluxi_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_sed_fluxi_out\"] = ((\"z\",\"lat\",\"sample\"), q_sed_fluxi_out[...,:split_index])\n",
    "my_dict_test[\"q_sed_fluxi_out\"] = ((\"z\",\"lat\",\"sample\"), q_sed_fluxi_out[...,split_index:])\n",
    "\n",
    "q_sed_fluxc_out = q_sed_fluxc_out.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "q_sed_fluxc_out = np.reshape(q_sed_fluxc_out, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"q_sed_fluxc_out\"] = ((\"z\",\"lat\",\"sample\"), q_sed_fluxc_out[...,:split_index])\n",
    "my_dict_test[\"q_sed_fluxc_out\"] = ((\"z\",\"lat\",\"sample\"), q_sed_fluxc_out[...,split_index:])\n",
    "\n",
    "terra = terra.transpose(\"z\",\"lat\",\"time\",\"lon\").values\n",
    "terra = np.reshape(terra, (n_z_input, n_y, n_files*n_x))\n",
    "my_dict_train[\"terra\"] = ((\"z\",\"lat\",\"sample\"), terra[...,:split_index])\n",
    "my_dict_test[\"terra\"] = ((\"z\",\"lat\",\"sample\"), terra[...,split_index:])\n",
    "\n",
    "sfc_pres = SFC_PRES.transpose(\"lat\",\"time\",\"lon\").values\n",
    "sfc_pres = np.reshape(sfc_pres, (n_y, n_files*n_x))\n",
    "my_dict_train[\"sfc_pres\"] = ((\"lat\",\"sample\"), sfc_pres[...,:split_index])\n",
    "my_dict_test[\"sfc_pres\"] = ((\"lat\",\"sample\"), sfc_pres[...,split_index:])\n",
    "\n",
    "skt = SKT.transpose(\"lat\",\"time\",\"lon\").values\n",
    "skt = np.reshape(skt, (n_y, n_files*n_x))\n",
    "my_dict_train[\"skt\"] = ((\"lat\",\"sample\"), skt[...,:split_index])\n",
    "my_dict_test[\"skt\"] = ((\"lat\",\"sample\"), skt[...,split_index:])\n",
    "\n",
    "cos_lat = np.expand_dims(cos_lat, axis=0)\n",
    "cos_lat = np.moveaxis(cos_lat, 2, 3)\n",
    "cos_lat = np.reshape(cos_lat, (1, n_y, -1)).squeeze()\n",
    "my_dict_train[\"cos_lat\"] = ((\"lat\",\"sample\"), cos_lat[...,:split_index])\n",
    "my_dict_test[\"cos_lat\"] = ((\"lat\",\"sample\"), cos_lat[...,split_index:])\n",
    "\n",
    "sin_lon = np.expand_dims(sin_lon, axis=0)\n",
    "sin_lon = np.moveaxis(sin_lon, 2, 3)\n",
    "sin_lon = np.reshape(sin_lon, (1, n_y, -1)).squeeze()\n",
    "my_dict_train[\"sin_lon\"] = ((\"lat\",\"sample\"), sin_lon[...,:split_index])\n",
    "my_dict_test[\"sin_lon\"] = ((\"lat\",\"sample\"), sin_lon[...,split_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_renormalization_factors(Tout, tflux_z, qtflux_z, qmic, qsed, rho_dz):\n",
    "    '''I need to renormalize somehow the outputs.\n",
    "     here I assume I am using a flux form, renormalizing (T_rad+rest , Tadv, qadv, qmic, qsed):'''\n",
    "\n",
    "    n_z = tflux_z.shape[0]\n",
    "    zTout = np.zeros(tflux_z.shape)\n",
    "    zqout = np.zeros(qtflux_z.shape)\n",
    "    zqsed = np.zeros(qsed.shape)\n",
    "    for k in range(n_z - 1):\n",
    "        zTout[k, :, :] = -(tflux_z[k + 1, :, :] - tflux_z[k, :, :]) / rho_dz[k]\n",
    "        zqout[k, :, :] = -(qtflux_z[k + 1, :, :] - qtflux_z[k, :, :]) / rho_dz[k]\n",
    "        zqsed[k, :, :] = -(qsed[k + 1, :, :] - qsed[k, :, :]) / rho_dz[k]\n",
    "\n",
    "    # flux is defined to be zero at top half-level\n",
    "    zTout[n_z - 1, :, :] = -(0.0 - tflux_z[n_z - 1, :, :]) / rho_dz[n_z - 1]\n",
    "    zqout[n_z - 1, :, :] = -(0.0 - qtflux_z[n_z - 1, :, :]) / rho_dz[n_z - 1]\n",
    "    zqsed[k, :, :] = -(0.0 - qsed[n_z - 1, :, :]) / rho_dz[n_z -1]\n",
    "\n",
    "    #Rescale humudity tendencies\n",
    "    zqsed = zqsed * atmos_physics.L / atmos_physics.cp\n",
    "    qmic = qmic * atmos_physics.L / atmos_physics.cp\n",
    "    zqout = zqout * atmos_physics.L / atmos_physics.cp\n",
    "\n",
    "    std1 = np.std(Tout)\n",
    "    std2 = np.std(zTout)\n",
    "    std3 = np.std(zqout)\n",
    "    std4 = np.std(qmic)\n",
    "    std5 = np.std(zqsed)\n",
    "\n",
    "    std_min = min(std1,std2,std3,std4,std5)\n",
    "    std1 = std1/std_min\n",
    "    std2 = std2 / std_min\n",
    "    std3 = std3 / std_min\n",
    "    std4 = std4 / std_min\n",
    "    std5 = std5 / std_min\n",
    "    return np.array([std1,std2,std3,std4,std5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_list = calculate_renormalization_factors(Tout[...,:split_index],\n",
    "                                                          T_adv_out[...,:split_index],\n",
    "                                                          q_adv_out[...,:split_index],\n",
    "                                                          q_auto_out[...,:split_index],\n",
    "                                                          q_sed_flux_tot[...,split_index:],\n",
    "                                                          rho_dz[:,0].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.31694353e+02, 4.08803907e+05, 6.31605873e+03, 1.95654958e+03,\n",
       "       1.00000000e+00])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_weight_dict['norms'] = ((\"norm\"), norm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = xr.Dataset(\n",
    "    my_dict_train,\n",
    "    coords={\n",
    "        \"z\": z[:n_z_input].values,\n",
    "        \"lat\": y.values,\n",
    "        \"sample\": np.arange(0,n_files*len(x.values), 1)[:split_index],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = xr.Dataset(\n",
    "    my_dict_test,\n",
    "    coords={\n",
    "        \"z\": z[:n_z_input].values,\n",
    "        \"lat\": y.values,\n",
    "        \"sample\": np.arange(0,n_files*len(x.values), 1)[split_index:],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = xr.Dataset(\n",
    "    my_weight_dict,\n",
    "    coords={\n",
    "        \"norm\":np.arange(1,6,1),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.to_netcdf(savepath + \"_train.nc\")\n",
    "ds_test.to_netcdf(savepath + \"_test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
