{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bbb906b-61d8-4eb6-ae6d-60a3250e76b8",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed68043-c7e4-4bac-a69a-bd91a6a28221",
   "metadata": {},
   "source": [
    "Run the \"training_test_generator_simple_big_data_for_memory_by_parts_numpy_v2.py\" file. I have bash scripts at path:\n",
    "\n",
    "/ocean/projects/ees220005p/gmooers/Githubs/Neural_nework_parameterization/NN_training/run_training/Training_data_Generators/Bash_Scripts\n",
    "\n",
    "For running the full 40 days of 12x coarse-grained data (found at /ocean/projects/ees220005p/gmooers/GM_Data) I reccomend splitting the data into 6 sections for the preprocessing. The bash scripts to do this are:\n",
    "\n",
    "bash_all_p1.sh\n",
    "bash_all_p2.sh\n",
    "bash_all_p3.sh\n",
    "bash_all_p4.sh\n",
    "bash_all_p5.sh\n",
    "bash_all_p6.sh\n",
    "\n",
    "where \"p1-p5\" will be the training data and I make \"p6\" the test data. Note that in p6 temporal shuffling is turned off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5572f-0293-47b0-aa3f-227a1c54e907",
   "metadata": {},
   "source": [
    "At this point 6 Netcdf files will be produced at \"/ocean/projects/ees220005p/gmooers/GM_Data/training_data/Training_Parts/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9af31-16f4-4f9a-bc9a-2d25576e9827",
   "metadata": {},
   "source": [
    "## Concatenate the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3046af-d235-43bf-a2e5-882a01639245",
   "metadata": {},
   "source": [
    "In directory \"/ocean/projects/ees220005p/gmooers/Githubs/Neural_nework_parameterization/NN_training/run_training/Training_data_Generators/Bash_Scripts\" submit concatenator.sh. This runs the python file \"train_files_concat.py\" in directory \"/ocean/projects/ees220005p/gmooers/Githubs/Neural_nework_parameterization/NN_training/src\" which will concatente the netcdf files together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e34c60-d164-414c-803d-c87a4839587a",
   "metadata": {},
   "source": [
    "## Convert the data to .zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a2a8f-5de2-45b2-9edc-ed1649561a71",
   "metadata": {},
   "source": [
    "In directory \"/ocean/projects/ees220005p/gmooers/Githubs/Neural_nework_parameterization/NN_training/run_training/Training_data_Generators/Bash_Scripts/\" submit the bash script \"data_type_changer.sh\" which runs \"data_type_conversion.py\" in \"/ocean/projects/ees220005p/gmooers/GM_Data/training_data/Training_Parts/\" which creates a single .zarr file for training and one for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47226f-be6b-4fd8-882d-3cb574ef1fa3",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb545241-07a6-49b5-8fef-546743696aa6",
   "metadata": {},
   "source": [
    "Use one of the bash scripts in \"/ocean/projects/ees220005p/gmooers/Githubs/Neural_nework_parameterization/NN_training/run_training/Improved_run_Experiments/Bash_Scripts\" submit it to queue. To specify the hyperparameters, data volume and model design, change the configurations in a yaml file in \"/ocean/projects/ees220005p/gmooers/Githubs/Neural_nework_parameterization/NN_training/run_training/Improved_run_Experiments/Config_Files/\" and make sure it is called to the bash script of your choice. \n",
    "\n",
    "You can specify where the model output is located in the savepath parameter of the yaml file but for now the default is \"/ocean/projects/ees220005p/gmooers/Investigations/Model_Performance\". Everything should be automatically generated by training the model. Is is also possible to use the yaml file to call a trained model and generate the post-processing information without retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23bfb9-fbc6-4e15-9b0f-f436e0e32d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
